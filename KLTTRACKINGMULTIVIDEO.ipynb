{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KLT TRACKING FOR MULTIPLE OBJECTS IN A GIVEN SCENE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. SETTING UP VIDEO CAPTURE AND PREPROCESSING FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Capture video from file or webcam (replace 'video.mp4' with your video file path)\n",
    "cap = cv2.VideoCapture('traffic3.mp4')\n",
    "\n",
    "# Parameters for ShiTomasi corner detection\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Parameters for Lucas-Kanade optical flow (KLT)\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Read the first frame\n",
    "ret, old_frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Cannot read video file\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# Convert the first frame to grayscale\n",
    "old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "103a19ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define train-test split ratio\n",
    "train_ratio = 0.8\n",
    "\n",
    "# Determine total frames in the video\n",
    "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "train_frames = int(total_frames * train_ratio)\n",
    "\n",
    "# Reset the video capture to the beginning\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 2.DETECTING FEATURES IN INITIAL FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Parameters for ShiTomasi corner detection (good features to track)\n",
    "feature_params = dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7)\n",
    "\n",
    "# Load the video\n",
    "cap = cv2.VideoCapture('traffic2.mp4')\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Read the first frame\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read the video frame.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# Convert frame to grayscale\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect features (key points) to track\n",
    "features = cv2.goodFeaturesToTrack(gray, mask=None, **feature_params)\n",
    "\n",
    "# Check if points are detected\n",
    "if features is None:\n",
    "    print(\"No features detected.\")\n",
    "    cap.release()\n",
    "    exit()\n",
    "\n",
    "# Draw the detected features as circles\n",
    "for point in features:\n",
    "    x, y = point.ravel()\n",
    "    cv2.circle(frame, (int(x), int(y)), 5, (0, 0, 255), -1)  # Green circles for features\n",
    "\n",
    "# Display the frame with features\n",
    "cv2.imshow('Feature Detection', frame)\n",
    "\n",
    "# Wait for key press and close window\n",
    "cv2.waitKey(0)\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c5027a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing video: Easy.mp4\n",
      "Completed processing video: Easy.mp4\n",
      "Processing video: traffic2.mp4\n",
      "Completed processing video: traffic2.mp4\n",
      "Processing video: traffic3.mp4\n",
      "Completed processing video: traffic3.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# List of video files to process\n",
    "video_files = ['Easy.mp4', 'traffic2.mp4', 'traffic3.mp4']\n",
    "\n",
    "for video_file in video_files:\n",
    "    print(f\"Processing video: {video_file}\")\n",
    "    \n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_file)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video {video_file}.\")\n",
    "        continue\n",
    "\n",
    "    # Calculate train-test split\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    train_frames = int(total_frames * train_ratio)\n",
    "\n",
    "    # Read the first frame for initial feature detection\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    ret, first_frame = cap.read()\n",
    "    if not ret:\n",
    "        print(f\"Error: Could not read the first frame of {video_file}.\")\n",
    "        cap.release()\n",
    "        continue\n",
    "\n",
    "    # Convert the first frame to grayscale for feature detection\n",
    "    first_gray = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
    "    features = cv2.goodFeaturesToTrack(first_gray, mask=None, **feature_params)\n",
    "\n",
    "    # Process frames for training segment\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "    for i in range(train_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Reached end of training segment for {video_file}.\")\n",
    "            break\n",
    "\n",
    "        # Insert tracking and feature update code here for training\n",
    "\n",
    "    # Process frames for testing segment\n",
    "    for i in range(train_frames, total_frames):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Reached end of testing segment for {video_file}.\")\n",
    "            break\n",
    "\n",
    "        # Insert tracking and feature update code here for testing\n",
    "\n",
    "    cap.release()\n",
    "    print(f\"Completed processing video: {video_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. TRACKING KEY POINTS ACROSS FRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture(\"traffic4.mp4\")  # Replace with your own video file\n",
    "\n",
    "# Check if video opened successfully\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video.\")\n",
    "    exit()\n",
    "\n",
    "# Parameters for optical flow\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Initialize the first frame\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Failed to read first frame.\")\n",
    "    exit()\n",
    "\n",
    "# Convert to grayscale for optical flow\n",
    "old_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Detect good features to track in the first frame\n",
    "initial_points = cv2.goodFeaturesToTrack(old_gray, mask=None, **dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7))\n",
    "\n",
    "# Initialize a mask for drawing the tracks\n",
    "mask = np.zeros_like(frame)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale for optical flow\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calculate optical flow to track points in the new frame\n",
    "    new_points, status, _ = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, initial_points, None, **lk_params)\n",
    "\n",
    "    # Select good points (points that were successfully tracked)\n",
    "    good_new = new_points[status == 1]\n",
    "    good_old = initial_points[status == 1]\n",
    "\n",
    "    # Re-detect points if necessary (when many points are lost)\n",
    "    if len(good_new) < 10:  # If fewer than 10 points are left, re-detect features\n",
    "        initial_points = cv2.goodFeaturesToTrack(frame_gray, mask=None, **dict(maxCorners=100, qualityLevel=0.3, minDistance=7, blockSize=7))\n",
    "        good_new = initial_points\n",
    "        good_old = good_new\n",
    "\n",
    "    # Draw tracking lines and points on the frame\n",
    "    for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = int(new[0]), int(new[1])  # Convert to integers\n",
    "        c, d = int(old[0]), int(old[1])  # Convert to integers\n",
    "        mask = cv2.line(mask, (a, b), (c, d), (0, 255, 0), 2)  # Green line for tracking\n",
    "        frame = cv2.circle(frame, (a, b), 5, (0, 0, 255), -1)  # Red circle for new points\n",
    "\n",
    "    # Overlay the mask on the frame\n",
    "    img = cv2.add(frame, mask)\n",
    "\n",
    "    # Display the tracking result\n",
    "    cv2.imshow(\"Tracking\", img)\n",
    "\n",
    "    # Update the previous frame and points\n",
    "    old_gray = frame_gray.copy()\n",
    "    initial_points = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    # Exit on 'q' key\n",
    "    if cv2.waitKey(30) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. DETECTING OBJECTS USING BACKGROUND SUBTRACTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I. SETTING UP BACKGROUND SUBTRACTOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture(\"traffic4.mp4\")  # Replace with your own video file\n",
    "\n",
    "# Initialize background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)\n",
    "\n",
    "# Parameters for KLT Tracker\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Initialize tracking points list\n",
    "tracking_points = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Apply background subtraction to detect moving objects\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "    \n",
    "    # Threshold the mask to binary\n",
    "    _, fg_mask = cv2.threshold(fg_mask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours of moving objects\n",
    "    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter contours based on size to avoid noise\n",
    "    bounding_boxes = []\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) > 500:  # Filter out small contours\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            bounding_boxes.append((x, y, w, h))\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw bounding box\n",
    "\n",
    "            # Initialize tracking points at the center of each bounding box\n",
    "            center = np.array([[x + w / 2, y + h / 2]], dtype=np.float32)\n",
    "            tracking_points.append(center)\n",
    "\n",
    "    cv2.imshow(\"Detection\", frame)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II.TRACKING THE DETECTED OBJECTS\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture(\"traffic3.mp4\")  # Replace with your own video file\n",
    "\n",
    "# Initialize background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)\n",
    "\n",
    "# Parameters for KLT Tracker\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Initialize tracking points and paths\n",
    "tracking_points = []\n",
    "tracking_paths = []\n",
    "\n",
    "# Read the first frame and initialize prev_gray\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read video.\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    prev_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert current frame to grayscale for optical flow\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply background subtraction to detect moving objects\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "    \n",
    "    # Threshold the mask to binary\n",
    "    _, fg_mask = cv2.threshold(fg_mask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours of moving objects\n",
    "    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter contours based on size to avoid noise\n",
    "    bounding_boxes = []\n",
    "    new_tracking_points = []\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) > 500:  # Filter out small contours\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            bounding_boxes.append((x, y, w, h))\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Draw bounding box in green\n",
    "\n",
    "            # Initialize tracking points at the center of each bounding box\n",
    "            center = np.array([[x + w / 2, y + h / 2]], dtype=np.float32)\n",
    "            new_tracking_points.append(center)\n",
    "\n",
    "    # Update tracking paths\n",
    "    if len(tracking_points) > 0:\n",
    "        # Calculate optical flow\n",
    "        new_points, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, gray_frame, tracking_points, None, **lk_params)\n",
    "        \n",
    "        # Draw lines for each tracked path\n",
    "        updated_tracking_paths = []\n",
    "        updated_tracking_points = []\n",
    "        for i, (new, old) in enumerate(zip(new_points, tracking_points)):\n",
    "            if status[i]:  # Only draw if the point was successfully tracked\n",
    "                a, b = new.ravel()\n",
    "                c, d = old.ravel()\n",
    "\n",
    "                # Only add a line segment every few frames to avoid dense trails\n",
    "                if len(tracking_paths[i]) == 0 or np.linalg.norm(np.array(tracking_paths[i][-1]) - np.array((a, b))) > 5:\n",
    "                    tracking_paths[i].append((a, b))\n",
    "                \n",
    "                # Prune paths to the last few points to keep them minimal\n",
    "                if len(tracking_paths[i]) > 2:\n",
    "                    tracking_paths[i] = tracking_paths[i][-2:]\n",
    "\n",
    "                # Draw the latest path segment\n",
    "                if len(tracking_paths[i]) == 2:\n",
    "                    pt1 = tuple(map(int, tracking_paths[i][0]))\n",
    "                    pt2 = tuple(map(int, tracking_paths[i][1]))\n",
    "                    cv2.line(frame, pt1, pt2, (255, 0, 255), 2)  # Draw tracking lines in purple\n",
    "                \n",
    "                # Keep only active points and paths\n",
    "                updated_tracking_paths.append(tracking_paths[i])\n",
    "                updated_tracking_points.append(new)\n",
    "\n",
    "        # Update tracking points and paths with only active ones\n",
    "        tracking_points = np.array(updated_tracking_points, dtype=np.float32).reshape(-1, 1, 2)\n",
    "        tracking_paths = updated_tracking_paths\n",
    "    else:\n",
    "        # Initialize tracking points and paths\n",
    "        tracking_points = np.array(new_tracking_points, dtype=np.float32)\n",
    "        tracking_paths = [[(p[0][0], p[0][1])] for p in tracking_points]\n",
    "\n",
    "    # Add new tracking points if they were not already being tracked\n",
    "    new_tracking_points = np.array(new_tracking_points, dtype=np.float32)\n",
    "    if len(new_tracking_points) > 0:\n",
    "        tracking_points = np.concatenate((tracking_points, new_tracking_points), axis=0)\n",
    "        tracking_paths.extend([[(p[0][0], p[0][1])] for p in new_tracking_points])\n",
    "\n",
    "    # Update the previous frame and tracking points\n",
    "    prev_gray = gray_frame.copy()\n",
    "    cv2.imshow(\"Detection and Tracking\", frame)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III. IMPROVING THE TRACKER (FINE TUNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load video\n",
    "cap = cv2.VideoCapture(\"traffic3.mp4\")  # Replace with your video path\n",
    "\n",
    "# Initialize background subtractor\n",
    "bg_subtractor = cv2.createBackgroundSubtractorMOG2(history=500, varThreshold=50, detectShadows=True)\n",
    "\n",
    "# Parameters for KLT Tracker\n",
    "lk_params = dict(winSize=(15, 15), maxLevel=2, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Initialize tracking points and paths\n",
    "tracking_points = np.array([], dtype=np.float32)\n",
    "tracking_paths = []  # Paths to store movement history\n",
    "\n",
    "# Set the maximum number of lines (cars) to track\n",
    "MAX_TRACKING_LINES = 10  # Track more cars\n",
    "MAX_PATH_LENGTH = 50  # Increase line length\n",
    "\n",
    "# Read the first frame and initialize prev_gray\n",
    "ret, frame = cap.read()\n",
    "if not ret:\n",
    "    print(\"Error: Could not read video.\")\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    prev_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert current frame to grayscale for optical flow\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply background subtraction to detect moving objects\n",
    "    fg_mask = bg_subtractor.apply(frame)\n",
    "\n",
    "    # Threshold the mask to binary\n",
    "    _, fg_mask = cv2.threshold(fg_mask, 250, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Find contours of moving objects\n",
    "    contours, _ = cv2.findContours(fg_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # Filter contours based on size to avoid noise\n",
    "    new_tracking_points = []\n",
    "    for cnt in contours:\n",
    "        if cv2.contourArea(cnt) > 500:  # Filter out small contours\n",
    "            x, y, w, h = cv2.boundingRect(cnt)\n",
    "            center = np.array([[x + w / 2, y + h / 2]], dtype=np.float32)  # Center of bounding box\n",
    "            new_tracking_points.append(center)\n",
    "\n",
    "            # Draw bounding box around the detected car\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)  # Green bounding box\n",
    "\n",
    "    # Update tracking points and paths if there are points\n",
    "    if len(tracking_points) > 0:\n",
    "        # Calculate optical flow\n",
    "        new_points, status, _ = cv2.calcOpticalFlowPyrLK(prev_gray, gray_frame, tracking_points, None, **lk_params)\n",
    "\n",
    "        # Update paths and draw tracking lines\n",
    "        updated_paths = []\n",
    "        updated_tracking_points = []\n",
    "        for i, (new, old) in enumerate(zip(new_points, tracking_points)):\n",
    "            if status[i]:\n",
    "                # Append new point to the existing path\n",
    "                tracking_paths[i].append((new[0][0], new[0][1]))\n",
    "                updated_paths.append(tracking_paths[i])\n",
    "                updated_tracking_points.append(new)\n",
    "\n",
    "                # Draw the tracking line for each car\n",
    "                for j in range(1, len(tracking_paths[i])):\n",
    "                    pt1 = (int(tracking_paths[i][j - 1][0]), int(tracking_paths[i][j - 1][1]))\n",
    "                    pt2 = (int(tracking_paths[i][j][0]), int(tracking_paths[i][j][1]))\n",
    "                    cv2.line(frame, pt1, pt2, (0, 0, 255), 2)  # Red line for tracking\n",
    "\n",
    "                # Limit path length for clarity\n",
    "                if len(tracking_paths[i]) > MAX_PATH_LENGTH:\n",
    "                    tracking_paths[i].pop(0)  # Remove the oldest point\n",
    "\n",
    "        tracking_paths = updated_paths  # Update paths with only active ones\n",
    "        tracking_points = np.array(updated_tracking_points, dtype=np.float32)  # Update active points\n",
    "\n",
    "    # Initialize new tracking points if no prior points exist\n",
    "    if len(tracking_points) == 0 and len(new_tracking_points) > 0:\n",
    "        tracking_points = np.array(new_tracking_points, dtype=np.float32)\n",
    "        tracking_paths = [[(p[0][0], p[0][1])] for p in new_tracking_points]\n",
    "\n",
    "    # Add new tracking points to current ones\n",
    "    elif len(new_tracking_points) > 0:\n",
    "        tracking_points = np.concatenate((tracking_points, new_tracking_points), axis=0)\n",
    "        tracking_paths.extend([[(p[0][0], p[0][1])] for p in new_tracking_points])\n",
    "\n",
    "    # Limit the number of tracking lines (cars) to MAX_TRACKING_LINES\n",
    "    if len(tracking_paths) > MAX_TRACKING_LINES:\n",
    "        tracking_paths = tracking_paths[:MAX_TRACKING_LINES]\n",
    "        tracking_points = tracking_points[:MAX_TRACKING_LINES]\n",
    "\n",
    "    # Update previous frame for optical flow\n",
    "    prev_gray = gray_frame.copy()\n",
    "\n",
    "    # Show the output\n",
    "    cv2.imshow(\"Detection and Tracking\", frame)\n",
    "\n",
    "    # Press 'q' to exit the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. CALCULATING THE ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected box 0 and Ground Truth box 0 IoU: 0.67\n",
      "Match found: Detected box 0 with Ground Truth box 0\n",
      "Detected box 1 and Ground Truth box 0 IoU: 0.00\n",
      "Detected box 1 and Ground Truth box 1 IoU: 0.68\n",
      "Match found: Detected box 1 with Ground Truth box 1\n",
      "\n",
      "Tracking accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2  # Corrected unpacking of box2\n",
    "\n",
    "    # Calculate the intersection coordinates\n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1 + w1, x2 + w2)\n",
    "    yi2 = min(y1 + h1, y2 + h2)\n",
    "\n",
    "    # Calculate the area of the intersection rectangle\n",
    "    intersection_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    # Calculate the area of both bounding boxes\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    # Calculate the union area\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = intersection_area / union_area if union_area != 0 else 0\n",
    "    return iou\n",
    "\n",
    "# Example detected and ground truth boxes (replace with actual data)\n",
    "detected_boxes = [\n",
    "    [100, 150, 80, 60],\n",
    "    [200, 250, 90, 70]\n",
    "]\n",
    "\n",
    "ground_truth_boxes = [\n",
    "    [105, 155, 85, 65],\n",
    "    [210, 260, 85, 65]\n",
    "]\n",
    "\n",
    "# Initialize counters\n",
    "correct_tracks = 0\n",
    "matched_gt_boxes = set()\n",
    "\n",
    "# Loop through detected and ground truth boxes\n",
    "for det_idx, det_box in enumerate(detected_boxes):\n",
    "    for gt_idx, gt_box in enumerate(ground_truth_boxes):\n",
    "        # Calculate IoU between detected and ground truth boxes\n",
    "        iou = calculate_iou(det_box, gt_box)\n",
    "        print(f\"Detected box {det_idx} and Ground Truth box {gt_idx} IoU: {iou:.2f}\")\n",
    "\n",
    "        # Check if IoU is above threshold and ground truth box has not been matched yet\n",
    "        if iou > 0.5 and gt_idx not in matched_gt_boxes:\n",
    "            correct_tracks += 1\n",
    "            matched_gt_boxes.add(gt_idx)  # Mark this GT box as matched\n",
    "            print(f\"Match found: Detected box {det_idx} with Ground Truth box {gt_idx}\")\n",
    "            break  # Stop checking this detected box once matched\n",
    "\n",
    "# Calculate total ground truth boxes and accuracy\n",
    "total_ground_truth = len(ground_truth_boxes)\n",
    "accuracy = correct_tracks / total_ground_truth if total_ground_truth > 0 else 0\n",
    "\n",
    "print(f\"\\nTracking accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. VISUALIZING THE ACCURACY OF THE TRACKER WITH THE RESPECTED VIDEO FOR OBJECT DETECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_iou(box1, box2):\n",
    "    x1, y1, w1, h1 = box1\n",
    "    x2, y2, w2, h2 = box2\n",
    "\n",
    "    # Calculate the intersection coordinates\n",
    "    xi1 = max(x1, x2)\n",
    "    yi1 = max(y1, y2)\n",
    "    xi2 = min(x1 + w1, x2 + w2)\n",
    "    yi2 = min(y1 + h1, y2 + h2)\n",
    "\n",
    "    # Calculate the area of the intersection rectangle\n",
    "    intersection_area = max(0, xi2 - xi1) * max(0, yi2 - yi1)\n",
    "\n",
    "    # Calculate the area of both bounding boxes\n",
    "    box1_area = w1 * h1\n",
    "    box2_area = w2 * h2\n",
    "\n",
    "    # Calculate the union area\n",
    "    union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "    # Calculate IoU\n",
    "    iou = intersection_area / union_area if union_area != 0 else 0\n",
    "    return iou\n",
    "\n",
    "# Example image (replace with an actual image)\n",
    "image = np.ones((500, 500, 3), dtype=np.uint8) * 255  # White background\n",
    "\n",
    "# Example detected and ground truth boxes\n",
    "detected_boxes = [\n",
    "    [100, 150, 80, 60],\n",
    "    [200, 250, 90, 70]\n",
    "]\n",
    "\n",
    "ground_truth_boxes = [\n",
    "    [105, 155, 85, 65],\n",
    "    [210, 260, 85, 65]\n",
    "]\n",
    "\n",
    "# Set IoU threshold\n",
    "iou_threshold = 0.5\n",
    "\n",
    "# Initialize counters\n",
    "correct_tracks = 0\n",
    "matched_gt_boxes = set()\n",
    "\n",
    "# Loop through detected and ground truth boxes\n",
    "for det_idx, det_box in enumerate(detected_boxes):\n",
    "    for gt_idx, gt_box in enumerate(ground_truth_boxes):\n",
    "        iou = calculate_iou(det_box, gt_box)\n",
    "\n",
    "        # Draw the ground truth boxes (green) and detected boxes (blue)\n",
    "        cv2.rectangle(image, (gt_box[0], gt_box[1]), (gt_box[0] + gt_box[2], gt_box[1] + gt_box[3]), (0, 255, 0), 2)\n",
    "        cv2.rectangle(image, (det_box[0], det_box[1]), (det_box[0] + det_box[2], det_box[1] + det_box[3]), (255, 0, 0), 2)\n",
    "\n",
    "        # If IoU is above threshold and the GT box is not matched\n",
    "        if iou > iou_threshold and gt_idx not in matched_gt_boxes:\n",
    "            correct_tracks += 1\n",
    "            matched_gt_boxes.add(gt_idx)  # Mark this GT box as matched\n",
    "\n",
    "            # Draw a label indicating the match\n",
    "            label = f\"Match {iou:.2f}\"\n",
    "            cv2.putText(image, label, (det_box[0], det_box[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_tracks / len(ground_truth_boxes) if len(ground_truth_boxes) > 0 else 0\n",
    "\n",
    "# Show the image with boxes and IoU labels\n",
    "cv2.imshow(\"IoU Visualization\", image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Print the tracking accuracy\n",
    "print(f\"Tracking accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. TESTING KLT FOR FACIAL RECOGNITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import cv2\\nimport numpy as np\\n\\n# Initialize the webcam\\ncap = cv2.VideoCapture(0)\\n\\n# Load the pre-trained Haar Cascade Classifier for face detection\\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\\n\\n# Termination criteria for the KLT tracker (Lucas-Kanade Optical Flow)\\nlk_params = dict(winSize=(21, 21), maxLevel=4, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\\n\\n# Variables for face tracking\\nface_roi = None\\ntracked_points = None\\nold_gray = None\\np0 = None\\n\\nwhile True:\\n    # Read a frame from the webcam\\n    ret, frame = cap.read()\\n    if not ret:\\n        break\\n\\n    # Convert the frame to grayscale\\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\\n\\n    # Detect faces in the frame\\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\\n\\n    # If a face is detected\\n    if len(faces) > 0:\\n        # Take the first detected face (you can add more logic to choose a specific one if needed)\\n        x, y, w, h = faces[0]\\n        face_roi = frame[y:y+h, x:x+w]\\n\\n        # Make the face bounding box more prominent by increasing thickness and using a different color\\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)  # Green color with 3px thickness\\n\\n        # Dynamically select feature points around the face using the detected face region\\n        step_size = 10  # Adjust step size to select more refined feature points\\n        feature_points = []\\n        for i in range(0, w, step_size):\\n            for j in range(0, h, step_size):\\n                feature_points.append((x + i, y + j))  # Feature points within the face region\\n\\n        # Convert feature points to numpy array for tracking\\n        tracked_points = np.array(feature_points, dtype=np.float32)\\n\\n        # Initialize the optical flow with the detected feature points if it's the first frame\\n        if old_gray is None and tracked_points is not None:\\n            old_gray = gray\\n            p0 = tracked_points.reshape(-1, 1, 2)\\n\\n    if tracked_points is not None and old_gray is not None:\\n        # Track the features using optical flow (Lucas-Kanade method)\\n        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, gray, p0, None, **lk_params)\\n\\n        # Ensure that optical flow was successfully computed (p1 is not None)\\n        if p1 is not None:\\n            # Select good points (tracked correctly)\\n            good_new = p1[st == 1]\\n            good_old = p0[st == 1]\\n\\n            # Draw the tracked points (as small plus signs) inside the bounding box\\n            for i, (new, old) in enumerate(zip(good_new, good_old)):\\n                a, b = new.ravel()\\n                # Ensure that point is inside the face region\\n                if x <= a <= x + w and y <= b <= y + h:\\n                    # Draw the white plus sign with improved visibility and sharpness\\n                    cv2.line(frame, (int(a) - 6, int(b)), (int(a) + 6, int(b)), (255, 255, 255), 2)  # Horizontal line\\n                    cv2.line(frame, (int(a), int(b) - 6), (int(a), int(b) + 6), (255, 255, 255), 2)  # Vertical line\\n\\n            # Update the previous frame and previous points for the next iteration\\n            old_gray = gray.copy()\\n            p0 = good_new.reshape(-1, 1, 2)\\n\\n    # Display the frame with face detection and feature tracking\\n    cv2.imshow('Face Detection and Feature Tracking', frame)\\n\\n    # Break the loop if the 'q' key is pressed\\n    if cv2.waitKey(1) & 0xFF == ord('q'):\\n        break\\n\\n# Release the webcam and close windows\\ncap.release()\\ncv2.destroyAllWindows()\\n\""
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load the pre-trained Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Termination criteria for the KLT tracker (Lucas-Kanade Optical Flow)\n",
    "lk_params = dict(winSize=(21, 21), maxLevel=4, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Variables for face tracking\n",
    "face_roi = None\n",
    "tracked_points = None\n",
    "old_gray = None\n",
    "p0 = None\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # If a face is detected\n",
    "    if len(faces) > 0:\n",
    "        # Take the first detected face (you can add more logic to choose a specific one if needed)\n",
    "        x, y, w, h = faces[0]\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Make the face bounding box more prominent by increasing thickness and using a different color\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)  # Green color with 3px thickness\n",
    "\n",
    "        # Dynamically select feature points around the face using the detected face region\n",
    "        step_size = 10  # Adjust step size to select more refined feature points\n",
    "        feature_points = []\n",
    "        for i in range(0, w, step_size):\n",
    "            for j in range(0, h, step_size):\n",
    "                feature_points.append((x + i, y + j))  # Feature points within the face region\n",
    "\n",
    "        # Convert feature points to numpy array for tracking\n",
    "        tracked_points = np.array(feature_points, dtype=np.float32)\n",
    "\n",
    "        # Initialize the optical flow with the detected feature points if it's the first frame\n",
    "        if old_gray is None and tracked_points is not None:\n",
    "            old_gray = gray\n",
    "            p0 = tracked_points.reshape(-1, 1, 2)\n",
    "\n",
    "    if tracked_points is not None and old_gray is not None:\n",
    "        # Track the features using optical flow (Lucas-Kanade method)\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, gray, p0, None, **lk_params)\n",
    "\n",
    "        # Ensure that optical flow was successfully computed (p1 is not None)\n",
    "        if p1 is not None:\n",
    "            # Select good points (tracked correctly)\n",
    "            good_new = p1[st == 1]\n",
    "            good_old = p0[st == 1]\n",
    "\n",
    "            # Draw the tracked points (as small plus signs) inside the bounding box\n",
    "            for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                a, b = new.ravel()\n",
    "                # Ensure that point is inside the face region\n",
    "                if x <= a <= x + w and y <= b <= y + h:\n",
    "                    # Draw the white plus sign with improved visibility and sharpness\n",
    "                    cv2.line(frame, (int(a) - 6, int(b)), (int(a) + 6, int(b)), (255, 255, 255), 2)  # Horizontal line\n",
    "                    cv2.line(frame, (int(a), int(b) - 6), (int(a), int(b) + 6), (255, 255, 255), 2)  # Vertical line\n",
    "\n",
    "            # Update the previous frame and previous points for the next iteration\n",
    "            old_gray = gray.copy()\n",
    "            p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    # Display the frame with face detection and feature tracking\n",
    "    cv2.imshow('Face Detection and Feature Tracking', frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.TRACKING THE FEATURES USING KLT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"import cv2\\nimport numpy as np\\n\\n# Initialize the webcam\\ncap = cv2.VideoCapture(0)\\n\\n# Load the pre-trained Haar Cascade Classifier for face detection\\nface_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\\n\\n# Termination criteria for the KLT tracker (Lucas-Kanade Optical Flow)\\nlk_params = dict(winSize=(21, 21), maxLevel=4, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\\n\\n# Variables for face tracking\\nface_roi = None\\ntracked_points = None\\nold_gray = None\\np0 = None\\ntracked_points_history = []  # To store previous points for drawing tracking lines\\n\\n# Define colors for different features (eyes, nose, lips, jaw)\\neye_color = (255, 0, 0)  # Blue for eyes\\nnose_color = (0, 255, 0)  # Green for nose\\nlip_color = (0, 0, 255)  # Red for lips\\njaw_color = (255, 255, 0)  # Cyan for jaw\\n\\nwhile True:\\n    # Read a frame from the webcam\\n    ret, frame = cap.read()\\n    if not ret:\\n        break\\n\\n    # Convert the frame to grayscale\\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\\n\\n    # Detect faces in the frame\\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\\n\\n    # If a face is detected\\n    if len(faces) > 0:\\n        # Take the first detected face (you can add more logic to choose a specific one if needed)\\n        x, y, w, h = faces[0]\\n        face_roi = frame[y:y+h, x:x+w]\\n\\n        # Make the face bounding box more prominent (black color with thicker line)\\n        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 0), 3)  # Black color with 3px thickness\\n\\n        # Dynamically select fewer feature points around the face using the detected face region\\n        step_size = 30  # Increased step size for fewer feature points\\n        feature_points = []\\n        for i in range(0, w, step_size):\\n            for j in range(0, h, step_size):\\n                feature_points.append((x + i, y + j))  # Feature points within the face region\\n\\n        # Convert feature points to numpy array for tracking\\n        tracked_points = np.array(feature_points, dtype=np.float32)\\n\\n        # Initialize the optical flow with the detected feature points if it's the first frame\\n        if old_gray is None and tracked_points is not None:\\n            old_gray = gray\\n            p0 = tracked_points.reshape(-1, 1, 2)\\n\\n    if tracked_points is not None and old_gray is not None:\\n        # Track the features using optical flow (Lucas-Kanade method)\\n        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, gray, p0, None, **lk_params)\\n\\n        # Ensure that optical flow was successfully computed (p1 is not None)\\n        if p1 is not None:\\n            # Select good points (tracked correctly)\\n            good_new = p1[st == 1]\\n            good_old = p0[st == 1]\\n\\n            # Color-code the tracked points based on the feature category\\n            for i, (new, old) in enumerate(zip(good_new, good_old)):\\n                a, b = new.ravel()\\n                # Ensure that point is inside the face region\\n                if x <= a <= x + w and y <= b <= y + h:\\n                    # Draw colored points (different colors for different features)\\n                    if i % 4 == 0:  # Eyes (blue)\\n                        color = eye_color\\n                    elif i % 4 == 1:  # Nose (green)\\n                        color = nose_color\\n                    elif i % 4 == 2:  # Lips (red)\\n                        color = lip_color\\n                    else:  # Jaw (cyan)\\n                        color = jaw_color\\n                    \\n                    # Draw the feature points (as small plus signs) with the corresponding color\\n                    cv2.line(frame, (int(a) - 8, int(b)), (int(a) + 8, int(b)), color, 3)  # Horizontal line\\n                    cv2.line(frame, (int(a), int(b) - 8), (int(a), int(b) + 8), color, 3)  # Vertical line\\n\\n                    # Track movement with lines (previous position to current)\\n                    if len(tracked_points_history) > 0:\\n                        prev_points = tracked_points_history[-1]\\n                        for j, prev in enumerate(prev_points):\\n                            if j < len(good_new):  # Ensure no index out of bounds\\n                                # Draw thicker and smoother lines between old and new positions\\n                                cv2.line(frame, (int(prev[0]), int(prev[1])), (int(good_new[j][0]), int(good_new[j][1])), color, 4)  # Smoother and bigger lines\\n\\n            # Update the tracked points history for drawing movement lines\\n            tracked_points_history.append(good_new)\\n            if len(tracked_points_history) > 15:  # Limit history to last 15 frames for smoother lines\\n                tracked_points_history.pop(0)\\n\\n            # Update the previous frame and previous points for the next iteration\\n            old_gray = gray.copy()\\n            p0 = good_new.reshape(-1, 1, 2)\\n\\n    # Display the frame with face detection, feature tracking, and color-coded features\\n    cv2.imshow('Face Detection and Movement Tracking', frame)\\n\\n    # Break the loop if the 'q' key is pressed\\n    if cv2.waitKey(1) & 0xFF == ord('q'):\\n        break\\n\\n# Release the webcam and close windows\\ncap.release()\\ncv2.destroyAllWindows()\\n\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Load the pre-trained Haar Cascade Classifier for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Termination criteria for the KLT tracker (Lucas-Kanade Optical Flow)\n",
    "lk_params = dict(winSize=(21, 21), maxLevel=4, criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Variables for face tracking\n",
    "face_roi = None\n",
    "tracked_points = None\n",
    "old_gray = None\n",
    "p0 = None\n",
    "tracked_points_history = []  # To store previous points for drawing tracking lines\n",
    "\n",
    "# Define colors for different features (eyes, nose, lips, jaw)\n",
    "eye_color = (255, 0, 0)  # Blue for eyes\n",
    "nose_color = (0, 255, 0)  # Green for nose\n",
    "lip_color = (0, 0, 255)  # Red for lips\n",
    "jaw_color = (255, 255, 0)  # Cyan for jaw\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the webcam\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the frame\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "\n",
    "    # If a face is detected\n",
    "    if len(faces) > 0:\n",
    "        # Take the first detected face (you can add more logic to choose a specific one if needed)\n",
    "        x, y, w, h = faces[0]\n",
    "        face_roi = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Make the face bounding box more prominent (black color with thicker line)\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 0), 3)  # Black color with 3px thickness\n",
    "\n",
    "        # Dynamically select fewer feature points around the face using the detected face region\n",
    "        step_size = 30  # Increased step size for fewer feature points\n",
    "        feature_points = []\n",
    "        for i in range(0, w, step_size):\n",
    "            for j in range(0, h, step_size):\n",
    "                feature_points.append((x + i, y + j))  # Feature points within the face region\n",
    "\n",
    "        # Convert feature points to numpy array for tracking\n",
    "        tracked_points = np.array(feature_points, dtype=np.float32)\n",
    "\n",
    "        # Initialize the optical flow with the detected feature points if it's the first frame\n",
    "        if old_gray is None and tracked_points is not None:\n",
    "            old_gray = gray\n",
    "            p0 = tracked_points.reshape(-1, 1, 2)\n",
    "\n",
    "    if tracked_points is not None and old_gray is not None:\n",
    "        # Track the features using optical flow (Lucas-Kanade method)\n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, gray, p0, None, **lk_params)\n",
    "\n",
    "        # Ensure that optical flow was successfully computed (p1 is not None)\n",
    "        if p1 is not None:\n",
    "            # Select good points (tracked correctly)\n",
    "            good_new = p1[st == 1]\n",
    "            good_old = p0[st == 1]\n",
    "\n",
    "            # Color-code the tracked points based on the feature category\n",
    "            for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "                a, b = new.ravel()\n",
    "                # Ensure that point is inside the face region\n",
    "                if x <= a <= x + w and y <= b <= y + h:\n",
    "                    # Draw colored points (different colors for different features)\n",
    "                    if i % 4 == 0:  # Eyes (blue)\n",
    "                        color = eye_color\n",
    "                    elif i % 4 == 1:  # Nose (green)\n",
    "                        color = nose_color\n",
    "                    elif i % 4 == 2:  # Lips (red)\n",
    "                        color = lip_color\n",
    "                    else:  # Jaw (cyan)\n",
    "                        color = jaw_color\n",
    "                    \n",
    "                    # Draw the feature points (as small plus signs) with the corresponding color\n",
    "                    cv2.line(frame, (int(a) - 8, int(b)), (int(a) + 8, int(b)), color, 3)  # Horizontal line\n",
    "                    cv2.line(frame, (int(a), int(b) - 8), (int(a), int(b) + 8), color, 3)  # Vertical line\n",
    "\n",
    "                    # Track movement with lines (previous position to current)\n",
    "                    if len(tracked_points_history) > 0:\n",
    "                        prev_points = tracked_points_history[-1]\n",
    "                        for j, prev in enumerate(prev_points):\n",
    "                            if j < len(good_new):  # Ensure no index out of bounds\n",
    "                                # Draw thicker and smoother lines between old and new positions\n",
    "                                cv2.line(frame, (int(prev[0]), int(prev[1])), (int(good_new[j][0]), int(good_new[j][1])), color, 4)  # Smoother and bigger lines\n",
    "\n",
    "            # Update the tracked points history for drawing movement lines\n",
    "            tracked_points_history.append(good_new)\n",
    "            if len(tracked_points_history) > 15:  # Limit history to last 15 frames for smoother lines\n",
    "                tracked_points_history.pop(0)\n",
    "\n",
    "            # Update the previous frame and previous points for the next iteration\n",
    "            old_gray = gray.copy()\n",
    "            p0 = good_new.reshape(-1, 1, 2)\n",
    "\n",
    "    # Display the frame with face detection, feature tracking, and color-coded features\n",
    "    cv2.imshow('Face Detection and Movement Tracking', frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
